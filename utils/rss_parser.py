import feedparser
from datetime import datetime
import re
import logging
import requests

logger = logging.getLogger(__name__)

def clean_html(raw_html):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML-—Ç–µ–≥–æ–≤"""
    return re.sub(r'<[^>]+>', '', str(raw_html or ''))

def get_source_meta(url):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫"""
    try:
        if 'reddit.com' in url:
            parts = url.split('/')
            subreddit = parts[4] if len(parts) > 4 else 'reddit'
            return f"Reddit/{subreddit}", "üë•"
        elif 'arxiv.org' in url:
            return "arXiv", "üìú"
        elif 'technologyreview.com' in url:
            return "MIT Tech Review", "üî¨"
        elif 'openai.com' in url:
            return "OpenAI", "ü§ñ"
        elif 'deepmind.com' in url:
            return "DeepMind", "üß†"
        else:
            domain = url.split('/')[2]
            return domain.replace('www.', ''), "üåê"
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {str(e)}")
        return "Unknown", "‚ùì"

def parse_rss(urls):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ RSS —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    entries = []
    if not urls:
        logger.warning("–ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ RSS-–ª–µ–Ω—Ç")
        return entries

    successful_sources = 0
    
    for url in urls:
        try:
            if not url.startswith('http'):
                logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–µ—Ä–Ω—ã–π URL: {url}")
                continue
                
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑: {url}")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è Reddit
            headers = {'User-Agent': 'Mozilla/5.0'} if 'reddit.com' in url else {}
            feed = feedparser.parse(url, request_headers=headers)
            
            if not feed.entries:
                logger.warning(f"–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –≤ {url}")
                continue
                
            source, emoji = get_source_meta(url)
            successful_sources += 1
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {source} ({url}), –Ω–∞–π–¥–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å–µ–π")
            
            # –ë–µ—Ä–µ–º –ø–æ 2 –Ω–æ–≤–æ—Å—Ç–∏ —Å –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞
            for entry in feed.entries[:2]:
                try:
                    title = clean_html(entry.get('title', ''))[:200] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                    description = clean_html(entry.get('summary', entry.get('description', '')))[:500]
                    link = entry.get('link', '')
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ —Å—Å—ã–ª–∫–∏ –∏–ª–∏ —Å —Å—Å—ã–ª–∫–æ–π –Ω–∞ —Å–∞–º RSS
                    if not link or link == url:
                        continue
                        
                    pub_date = entry.get('published', '')
                    entries.append({
                        'title': f"{emoji} {title}",
                        'description': description,
                        'source': source,
                        'url': link,
                        'date': pub_date if pub_date else datetime.now().isoformat()
                    })
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏ –∏–∑ {source}: {str(e)}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {str(e)}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–µ (—Å–≤–µ–∂–∏–µ —Å–Ω–∞—á–∞–ª–∞)
    entries.sort(key=lambda x: x.get('date', ''), reverse=True)
    
    logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {successful_sources}/{len(urls)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –≤—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(entries)}")
    return entries